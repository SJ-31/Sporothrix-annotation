{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUSCO\n",
    "- Mandatory arguments\n",
    "    - `busco -i [SEQUENCE_FILE] -l [LINEAGE] -o [OUTPUT_NAME] -m [MODE] [OTHER OPTIONS]`\n",
    "- -i or --in defines the input file to analyse which is either a nucleotide fasta file or a protein fasta file, depending on the BUSCO mode. As of v5.1.0 the input argument can now also be a directory containing fasta files to run in batch mode.\n",
    "- -o or --out defines the folder that will contain all results, logs, and intermediate data\n",
    "- -m or --mode sets the assessment MODE: genome, proteins, transcriptome\n",
    "- -l or --lineage_dataset\n",
    "    - It can be a dataset name, i.e. bacteria_odb10, or a path i.e. ./bacteria_odb10 or /home/user/bacteria_odb10. In the former case, which is the recommended usage, BUSCO will automatically download and version the corresponding dataset. In the latter case, the dataset found in the given path will be used. Lineage can be ignored if running automated lineage selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>complete_x</th>\n",
       "      <th>Assembler_x</th>\n",
       "      <th>complete_y</th>\n",
       "      <th>Assembler_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S01</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>spades</td>\n",
       "      <td>1251.0</td>\n",
       "      <td>megahit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S02</td>\n",
       "      <td>3423.0</td>\n",
       "      <td>spades</td>\n",
       "      <td>3352.0</td>\n",
       "      <td>megahit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S03</td>\n",
       "      <td>3626.0</td>\n",
       "      <td>spades</td>\n",
       "      <td>3641.0</td>\n",
       "      <td>megahit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S04</td>\n",
       "      <td>3595.0</td>\n",
       "      <td>spades</td>\n",
       "      <td>3621.0</td>\n",
       "      <td>megahit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S05</td>\n",
       "      <td>3025.0</td>\n",
       "      <td>spades</td>\n",
       "      <td>3006.0</td>\n",
       "      <td>megahit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>S06</td>\n",
       "      <td>3659.0</td>\n",
       "      <td>spades</td>\n",
       "      <td>3655.0</td>\n",
       "      <td>megahit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S07</td>\n",
       "      <td>3647.0</td>\n",
       "      <td>spades</td>\n",
       "      <td>3668.0</td>\n",
       "      <td>megahit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S08</td>\n",
       "      <td>3674.0</td>\n",
       "      <td>spades</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>megahit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S09</td>\n",
       "      <td>3318.0</td>\n",
       "      <td>spades</td>\n",
       "      <td>3490.0</td>\n",
       "      <td>megahit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S10</td>\n",
       "      <td>3669.0</td>\n",
       "      <td>spades</td>\n",
       "      <td>3661.0</td>\n",
       "      <td>megahit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>S11</td>\n",
       "      <td>3669.0</td>\n",
       "      <td>spades</td>\n",
       "      <td>3675.0</td>\n",
       "      <td>megahit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>S12</td>\n",
       "      <td>1333.0</td>\n",
       "      <td>spades</td>\n",
       "      <td>1319.0</td>\n",
       "      <td>megahit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>S13</td>\n",
       "      <td>1081.0</td>\n",
       "      <td>spades</td>\n",
       "      <td>1291.0</td>\n",
       "      <td>megahit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>S14</td>\n",
       "      <td>1370.0</td>\n",
       "      <td>spades</td>\n",
       "      <td>1572.0</td>\n",
       "      <td>megahit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>S15</td>\n",
       "      <td>26.0</td>\n",
       "      <td>spades</td>\n",
       "      <td>20.0</td>\n",
       "      <td>megahit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>S16</td>\n",
       "      <td>2452.0</td>\n",
       "      <td>spades</td>\n",
       "      <td>3432.0</td>\n",
       "      <td>megahit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sample  complete_x Assembler_x  complete_y Assembler_y\n",
       "0     S01      1297.0      spades      1251.0     megahit\n",
       "1     S02      3423.0      spades      3352.0     megahit\n",
       "2     S03      3626.0      spades      3641.0     megahit\n",
       "3     S04      3595.0      spades      3621.0     megahit\n",
       "4     S05      3025.0      spades      3006.0     megahit\n",
       "5     S06      3659.0      spades      3655.0     megahit\n",
       "6     S07      3647.0      spades      3668.0     megahit\n",
       "7     S08      3674.0      spades      3673.0     megahit\n",
       "8     S09      3318.0      spades      3490.0     megahit\n",
       "9     S10      3669.0      spades      3661.0     megahit\n",
       "10    S11      3669.0      spades      3675.0     megahit\n",
       "11    S12      1333.0      spades      1319.0     megahit\n",
       "12    S13      1081.0      spades      1291.0     megahit\n",
       "13    S14      1370.0      spades      1572.0     megahit\n",
       "14    S15        26.0      spades        20.0     megahit\n",
       "15    S16      2452.0      spades      3432.0     megahit"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "import pandas as pd\n",
    "spades = pd.read_csv('results/assembly/4-assess/busco/spades/multiqc/multiqc_data/multiqc_busco.txt', sep='\\t').drop('lineage_dataset', axis='columns')\n",
    "spades = spades.drop(['complete_single_copy', 'missing', 'total', 'complete_duplicated', 'fragmented'], axis='columns')\n",
    "megahit = pd.read_csv('results/assembly/4-assess/busco/megahit/multiqc/multiqc_data/multiqc_busco.txt', sep='\\t').drop('lineage_dataset', axis='columns')\n",
    "megahit = megahit.drop(['complete_single_copy', 'missing', 'total', 'complete_duplicated', 'fragmented'], axis='columns')\n",
    "for df in [spades, megahit]:\n",
    "    extracted = df['Sample'].str.extract(r'.*(S.*)_(.*)')\n",
    "    df['Sample'] = extracted[0]\n",
    "    df['Assembler'] = extracted[1]\n",
    "pd.merge(spades, megahit, on='Sample')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKER\n",
    "Annotation by maker proceeds in several rounds, each one building upon the last. \n",
    "Based on [this](https://github.com/Joseph7e/MAKER-genome-annotations-tutorial) tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing repeat database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before annotation, it is important to prepare a repeat database to prevent gene predictors from including known repeats (e.g. LTRs, tandem repeats) in their analyses. Since the standard resource for this, RepBase, is now behind a paywall, we need to prepare our own species-specific database. This will be carried out with the software RepeatModeler and the *Sporothrix schenckii* reference genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BuildDatabase -name <name> reference-genome.fasta\n",
    "RepeatModeler -database <species blast database> -LTRStruct "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since repeat modeler having identified ... repeat elements, you'll have to use additional tools to increase the size of the repeat element library.\n",
    "- EDTA \n",
    "    - will be run with the`--curatedlib` flag set to the (identified) output of RepeatModeler.\n",
    "- MITE hunter\n",
    " - This uses the genome only\n",
    " - \n",
    "- TransposonPSI\n",
    "- Downloaded a curated transposable element sequences from [TREP](https://trep-db.uzh.ch/index.php) database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDTA\n",
    "perl EDTA.pl --genome S_schenckii_genome.fasta --curatedlib <repeatmodeler file> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 1\n",
    "In the first round, set maker to infer gene predictions from all ESTs and from protein alignments. \n",
    "- Use the `gff3` file output to train augustus and snap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "est2genome=1 #infer gene predictions directly from ESTs, 1 = yes, 0 = no\n",
    "protein2genome=1 #infer predictions from protein homology, 1 = yes, 0 = no"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the second round, Galaxy recommends to train the predictors and second time using the output of the maker second run. Once done, just rerun Maker a third time using the same params as the second round. \n",
    "- Since there will be multiple gff files though for each contig, we'll instead merge the files together, then extract the est, protein and repeat element annotations into separate files for use instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gff3_merge -n -s -d <sample>_master_datastore_index.log > <sample>.all.maker.noseq.gff\n",
    "# transcript alignments\n",
    "awk '{ if ($2 == \"est2genome\") print $0 }' ${sample}.all.maker.noseq.gff > all.maker.est2genome.gff\n",
    "# protein alignments\n",
    "awk '{ if ($2 == \"protein2genome\") print $0 }' .all.maker.noseq.gff > .all.maker.protein2genome.gff\n",
    "# repeat alignments\n",
    "awk '{ if ($2 ~ \"repeat\") print $0 }' .all.maker.noseq.gff > .all.maker.repeats.gff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#-----Re-annotation Using MAKER Derived GFF3\n",
    "maker_gff= #MAKER derived GFF3 file, output of the previous maker run\n",
    "est_pass=1 #use ESTs in maker_gff: 1 = yes, 0 = no\n",
    "altest_pass=1 #use alternate organism ESTs in maker_gff: 1 = yes, 0 = no \n",
    "protein_pass=1 #use protein alignments in maker_gff: 1 = yes, 0 = no\n",
    "rm_pass=1 #use repeats in maker_gff: 1 = yes, 0 = no\n",
    "model_pass=0 #use gene models in maker_gff: 1 = yes, 0 = no\n",
    "pred_pass=0 #use ab-initio predictions in maker_gff: 1 = yes, 0 = no\n",
    "other_pass=0 #passthrough anyything else in maker_gff: 1 = yes, 0 = no\n",
    "\n",
    "# Set both the snap and augustus lines to the output of the training\n",
    "#-----Gene Prediction\n",
    "snaphmm= #SNAP HMM file \n",
    "gmhmm= #GeneMark HMM file \n",
    "augustus_species= #Augustus gene prediction species model\n",
    "fgenesh_par_file= #FGENESH parameter file\n",
    "pred_gff= #ab-initio predictions from an external GFF3 file\n",
    "model_gff= #annotated gene models from an external GFF3 file (annotation pass-through)\n",
    "run_evm=0 #run EvidenceModeler, 1 = yes, 0 = no\n",
    "# Will also disable these\n",
    "est2genome=0 #infer gene predictions directly from ESTs, 1 = yes, 0 = no\n",
    "protein2genome=0 #infer predictions from protein homology, 1 = yes, 0 = no\n",
    "#\n",
    "trna=0 #find tRNAs with tRNAscan, 1 = yes, 0 = no\n",
    "snoscan_rrna= #rRNA file to have Snoscan find snoRNAs\n",
    "snoscan_meth= #-O-methylation site fileto have Snoscan find snoRNAs\n",
    "unmask=0 #also run ab-initio prediction programs on unmasked sequence, 1 = yes, 0 = no\n",
    "allow_overlap= #allowed gene overlap fraction (value from 0 to 1, blank for default)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config files\n",
    "- Run `maker -CTL` to generate all the config files\n",
    "- Set `model_org=` to empty until we find a way to get RepBase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output file info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Ab initio* gene prediction\n",
    "To train the predictors, pass it the original genome file, as well as the fasta output from the first round of maker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNAP\n",
    "***The following describes steps from maker output***\n",
    "- Preparation\n",
    "\t- Convert the GFF3 gene models into ZFF format\n",
    "\t- Make a directory, move into it, and run `gff3_merge -d` using the datastore index log as an argument to merge the gff files into one\n",
    "\t- Then run `maker2zff`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "mkdir <snap directory>\n",
    "cd <snap directory>\n",
    "gff3_merge -d <master_datastore_index.log>\n",
    "maker2zff <the merged gff file> # Creates .ann and .dna files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "fathom -categorize <intergenic sequence limit> <genome>.ann <genome>.dna\n",
    "fathom <>.ann <>.dna -gene-stats > gene-stats.log 2>&1 # Get stats\n",
    "fathom <>.ann <>.dna -validate > validate.log 2>&1 # Validate\n",
    "fathom -export 1000 -plus uni.ann uni.dna # plus converts the sequence to plus strand \n",
    "forge export.ann export.dna\n",
    "hmm-assembler.pl <name> <file directory from forge export, usually current dir>  > <training parameters file name>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training parameter file has been created, update the `maker_opts.ctl` file, setting the following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "snaphmm=<training file name>\n",
    "est2genome=0\n",
    "protein2genome=0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augustus\n",
    "Protocol from https://biohpc.cornell.edu/doc/annotation_2019_exercises1_v2.pdf\n",
    "- To start off, create a merged gff file the same way you did for snap\n",
    "- Next, filter the file so it only contains maker annotations  \n",
    "- Convert the maker gff and fasta file into a genbank formatted file, keeping 2000 np upstream and downstream of each gene for model training\n",
    "- Create a new augustus species, then perform initial training using the genbank file \n",
    "- You can check the number of genes in the training set using `grep -c LOCUS <gb file>`\n",
    "- **Model training**\n",
    "    - Create a new augustus species, then train it on the gb file as that species. It will be stored in the default augustus/config directory\n",
    "    - Perform the initial training\n",
    "    - Create a smaller test set (by splitting the gb file) for evaluating the model before and after obptimization\n",
    "        - Use the first model to predict genes in the test set \n",
    "    - **Model optimization (very time consuming)**\n",
    "        - To speed things up, you can train the model with a smaller test set \n",
    "        - This test set will be splitted into 24 kfolds for optimization (the kfold can be set up to 48, with processed with one cpu core per kfold. Kfold must be same number as as cpus). \n",
    "        - The training, prediction and evaluation will be performed on each bucket in parallel (training on hh.gb.train+each bucket, then comparing each bucket with the union of the rest). \n",
    "        - By default, 5 rounds of optimization. As optimization for large genome could take days, I changed it to 3 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Option 2\n",
    "gff3_merge -d <datastore_index.log>\n",
    "awk '{if ($2==\"maker\") print }' <all.gff> > filtered_file.gff \n",
    "gff2gbSmallDNA.pl <gff file> <assembly fasta file> 2000 <genbank filename>.gb\n",
    "new_species.pl --species=<your_species>\n",
    "\n",
    "    # Training\n",
    "etraining --species=<your_species> <genbank file>\n",
    "randomSplit.pl <genbank file> <number of genes to split> # Yields a file called \"<gb file name>.test\" \n",
    "mv <filename>.gb.test <filename>.gb.evaluation # Rename for evaluation\n",
    "augustus --species=<your_species> .gb.evaluation >& first_evaluation.out # First model testing\n",
    "grep -A 22 Evaluation first_evaluation.out # Check results\n",
    "\n",
    "# Optimize\n",
    "randomSplit.pl <genbank file> 1000\n",
    "optimize_augustus.pl --species=<your species> --kfold=24 --cpus=24 --rounds=3 --onlytrain=<filename>.gb.train <filename>.gb.test >& log\n",
    "\n",
    "# Train after optimization\n",
    "etraining --species=<your_species> <genbank file>\n",
    "\n",
    "# Testing the optimized model\n",
    "augustus --species=<your_species> .gb.evaluation >& second_evaluation.out # Second model testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative: Training Augustus with BUSCO\n",
    "First slice of the ends of mRNA annotations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GenemarksES\n",
    "All you need for this is the geome assembly file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gmes_petap.pl -ES -sequence <assembly.fasta>  -fungus\n",
    "# The target file is gmhmm.mod (it produces a bunch of files in the current directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "Look into repeat library construction\n",
    "- https://weatherby.genetics.utah.edu/MAKER/wiki/index.php/Repeat_Library_Construction-Basic\n",
    "- https://weatherby.genetics.utah.edu/MAKER/wiki/index.php/Repeat_Library_Construction-Advanced"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref sequences\n",
    "\n",
    "## Reference genomes\n",
    "Used in construction of Repeat library and for training Augustus\n",
    "- NCBI RefSeq assembly GCF_000961545.1\n",
    "\n",
    "- Additional repeat library construction\n",
    "\n",
    "\n",
    "## Protein evidence for maker\n",
    "- Search for \"Sporothrix\" on UniProtKB, Mon 15 May, 2023, Release 2023_02 \n",
    "\n",
    "## Transcriptome \n",
    "Initial database: all Sporothrix sequences from the NCBI's Nucleotide database, \n",
    "- NCBI nucleotide database search query: \"txid29907[Organism:exp] AND biomol_mrna[PROP]\" Mon 15 May, 2023 \n",
    "- A transcriptome assembled using rnaspades SRA accession number SRX6367452"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "https://darencard.net/blog/2017-05-16-maker-genome-annotation/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
